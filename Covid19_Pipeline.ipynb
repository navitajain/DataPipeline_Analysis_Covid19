{"cells":[{"cell_type":"markdown","source":["### PART 1 - Data pipeline with only Spark's 1-Master and 1-Worker node. main to start ETL"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import DataFrame\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport pandas as pd\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["## ETL - get Covid date from date range \ndef end_date():\n  cdate = spark.sql(\"\"\" select current_date() as cdate \"\"\").collect()[0]['cdate'].strftime(\"%m-%d-%Y\") \n  end_date = (datetime.strptime(cdate, '%m-%d-%Y') - timedelta(days=1)).strftime('%m-%d-%Y')\n  return end_date\n\ndef start_date():\n  \"\"\"\n  retreive the max date from the log file. It's a transactional file that keeps infrmation of the dates for which data has been successfully   download\n  \"\"\" \n  try:\n      dbutils.fs.ls(\"dbfs:/Covid_datasets/log\")\n      max_date = spark.sql(\"SELECT max(*) FROM csv.`dbfs:/Covid_datasets/log`\").collect()\n      start_date = (datetime.strptime(max_date[0][0], '%m-%d-%Y') + timedelta(days=1)).strftime('%m-%d-%Y')\n  except:\n      start_date = \"01-22-2020\"\n  return start_date"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["# date range for which the data willbe pulled from github respository\ndef dateRange():\n  return(pd.date_range(start = start_date(), end = end_date()))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Transform data, create a aggregated sum of cases found grouped Country wise\ndef transform():\n    global df_conf\n    global df_death\n    global df_recover\n    \n    ## Load data into dataframes \n    df_conf = spark.createDataFrame([], schema)\n    df_death = spark.createDataFrame([], schema)\n    df_recover = spark.createDataFrame([], schema)\n    \n    print(\"IN transform\")\n\n    for d in date_range:\n        date = d.strftime(\"%m-%d-%Y\")\n        file = \"file:/tmp/Covid_datasets/\"+str(date)+\".csv\"\n        print('Transform file: ',file)\n        df_data = spark.read.format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat') \\\n                 .option('header','true')\\\n                 .option('inferSchema','true')\\\n                 .load(file)\n    \n        # files dated older than 03-23 has different column name.\n        if 'Country/Region' in df_data.columns:\n            df_data = df_data.withColumnRenamed('Country/Region','Country_Region')\n            \n        try:   \n            df_clean = df_data['Country_Region','Confirmed','Deaths','Recovered']\n        except Exception as e:\n            print(e)\n        \n        \n        # group by country column and sum the cases for columns to give daily cumulative records\n        df_table = df_clean.groupBy('Country_Region').sum()\n        \n        # join all the files/tables that were batch extracted on current run\n        conf_table = df_table['Country_Region','sum(Confirmed)'].withColumnRenamed('sum(Confirmed)',date)\n        df_conf = df_conf.join(conf_table, on=\"Country_Region\", how='full')\n\n        death_table = df_table['Country_Region','sum(Deaths)'].withColumnRenamed('sum(Deaths)',date)\n        df_death = df_death.join(death_table, on=\"Country_Region\", how='full')\n\n        recover_table = df_table['Country_Region','sum(Recovered)'].withColumnRenamed('sum(Recovered)',date)\n        df_recover = df_recover.join(recover_table, on=\"Country_Region\", how='full')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["# renaming columns in the dataframe\ndef rename_columns(df):\n    old_cols = df.columns\n    new_cols = [f.strip('sum(').strip(')') for f in df.columns]\n    rename = list(zip(old_cols,new_cols))\n    for old,new in rename:\n          df = df.withColumnRenamed(old,new)\n    return df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# store data in some dbfs and add to it the daily updates\ndef load():\n    print(\"IN LOAD\")    \n\n    # The schema/structure of the data is to have a time-series data with dates on columns and each row is a country\n    \n    # command with absolute path\n    df_conf.write.option(\"mergeSchema\",\"true\").format(\"delta\").mode(\"append\").save(\"dbfs:/Covid_datasets/Covid_Confirmed\")\n    df_death.write.option(\"mergeSchema\",\"true\").format(\"delta\").mode(\"append\").save(\"dbfs:/Covid_datasets/Covid_Deaths\")\n    df_recover.write.option(\"mergeSchema\",\"true\").format(\"delta\").mode(\"append\").save(\"dbfs:/Covid_datasets/Covid_Recovered\")\n\n\n    # The above command appends to the table and creates a new row for exisiting Country. \n    # Groupby Countrys and sum the rows and write back to dbfs\n    delta_files = [\"dbfs:/Covid_datasets/Covid_Confirmed\",\"dbfs:/Covid_datasets/Covid_Deaths\",\"dbfs:/Covid_datasets/Covid_Recovered\"]\n    for file in delta_files:\n        # groupby and sum new appended row of exisiting country_region\n        df = spark.read.format(\"delta\").load(file).groupby('Country_Region').sum()\n        # suming the rows changes the column names to sum('')\n        df = rename_columns(df)\n        df.write.format(\"delta\").option(\"overwriteSchema\",\"true\").mode(\"overwrite\").save(file)\n  \n    \n    print('Covid_Confirmed, Covid_Deaths and Covid_Recovered tables created')\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["``` We are extracting data from https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/\ninstead of this link https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series\n    .. because it has cumulative numbers rather than just day by day counts.```"],"metadata":{}},{"cell_type":"code","source":["def extract():\n    # mine/get all .csv files in date range from github daily reports url\n    print('Fetching data to local cluster........')\n    for d in date_range:\n        dated = d.strftime(\"%m-%d-%Y\")\n        file_dated = str(dated) +'.csv'\n        url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/\" + file_dated\n        try:\n          # extract all the files to local disk\n          !wget -P /tmp/Covid_datasets \"$url\"\n          localpath = \"file:/tmp/Covid_datasets/\" + file_dated\n          print('Written file to databricks local filesystem: ', file_dated)          \n        except Exception as e:\n          print(e)       "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["## Initial ETL pipeline \ndef ETL_Pipeline():\n  \n    global date_range\n    date_range = dateRange()\n\n    # mine/get all .csv files in date range from github daily reports url\n    extract()\n\n    # transform/process data from each file\n    transform()\n\n    # load date in specified dataframes\n    load()\n \n    # write to log all the dates whose data is downloaded\n    mylist = list(date_range.map(lambda x: x.strftime(\"%m-%d-%Y\")))\n    spark.createDataFrame(mylist, StringType()).write.format(\"csv\").mode(\"append\").save(\"dbfs:/Covid_datasets/log\")\n\n    # recursively delete all files and folders in the directory releasing memory once copied to dbgs #freeUpFile(file)\n    print(\"Finally, freeing the space by deleting all the files download from git\")\n    dbutils.fs.rm(\"file:/tmp/Covid_datasets\",True) "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["if __name__ == \"__main__\":\n    \"\"\"starts the pipeline, extracts data, transforms it and loads it into a dbfs client\"\"\"\n    # global empty dataframes\n    global df_conf\n    global df_death\n    global df_recover\n    # schema for the global dataframe\n    schema = StructType([StructField(\"Country_Region\", StringType(), True)])\n\n    ## create a separate folder in dbfs filesystem for Covid19 dataset \n    try:\n      dbutils.fs.ls(\"dbfs:/Covid_datasets\")\n    except:\n      dbutils.fs.mkdirs(\"dbfs:/Covid_datatsets\")\n    #create a new directory in local filesystem as staging area\n    try:\n      dbutils.fs.rm(\"file:/tmp/Covid_datasets\",True)\n    except:\n      pass\n    \n    dbutils.fs.mkdirs(\"file:/tmp/Covid_datasets/\")\n\n    # date range for which the data willbe pulled from github respository\n    global date_range\n\n    # Extract, trasnform and load data in DBFS from github's covid updated data master branch\n    ETL_Pipeline()\n    \n    dbutils.fs.rm(\"file:/tmp/Covid_datasets\",True)\n    \n    "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Fetching data to local cluster........\nWritten file to databricks local filesystem:  04-17-2020.csv\nWritten file to databricks local filesystem:  04-18-2020.csv\nWritten file to databricks local filesystem:  04-19-2020.csv\nWritten file to databricks local filesystem:  04-20-2020.csv\nIN transform\nTransform file:  file:/tmp/Covid_datasets/04-17-2020.csv\nTransform file:  file:/tmp/Covid_datasets/04-18-2020.csv\nTransform file:  file:/tmp/Covid_datasets/04-19-2020.csv\nTransform file:  file:/tmp/Covid_datasets/04-20-2020.csv\nIN LOAD\nCovid_Confirmed, Covid_Deaths and Covid_Recovered tables created\nFinally, freeing the space by deleting all the files download from git\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["### Clean space\n###set spark setting to spark.databricks.delta.retentionDurationCheck.enabled = False to enable VACUUM-ing with retaining data < 100hours"],"metadata":{}},{"cell_type":"code","source":["\n%sql\n%md #VACUUM 'dbfs:/Covid_datasets/Covid_Confirmed' DRY RUN\n\nVACUUM delta.'dbfs:/Covid_datasets/Covid_Confirmed' RETAIN 48 HOURS\nVACUUM delta.'dbfs:/Covid_datasets/Covid_Deaths' RETAIN 48 HOURS\nVACUUM delta.'dbfs:/Covid_datasets/Covid_Recovered' RETAIN 48 HOURS\nVACUUM delta.'dbfs:/Covid_datasets/log' RETAIN 48 HOURS"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["#### display the memory usage by files in databricks"],"metadata":{}},{"cell_type":"code","source":["%sh\nexec <&- 2> /dev/null\necho \"=Look for big files:\"\ndu --human-readable --max-depth=2 --apparent-size --exclude='/dbfs/mnt' \\\n    --exclude='/dbfs/databricks-*' /dbfs\necho\necho \"=Look for big local files:\"\ndu --human-readable --max-depth=1 --exclude='/dbfs' /"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">=Look for big files:\n15M\t/dbfs/Covid_datasets/Covid_Confirmed\n15M\t/dbfs/Covid_datasets/Covid_Deaths\n15M\t/dbfs/Covid_datasets/Covid_Recovered\n3.3K\t/dbfs/Covid_datasets/log\n44M\t/dbfs/Covid_datasets\n0\t/dbfs/Covid_datatsets\n0\t/dbfs/FileStore/import-stage\n9.2M\t/dbfs/FileStore/plots\n17M\t/dbfs/FileStore/tables\n27M\t/dbfs/FileStore\n1.7K\t/dbfs/datasets\n558\t/dbfs/delta/events\n558\t/dbfs/delta\n1.1K\t/dbfs/local_disk0/tmp\n1.1K\t/dbfs/local_disk0\n4.0K\t/dbfs/ml\n0\t/dbfs/tmp/hive\n0\t/dbfs/tmp\n626K\t/dbfs/user/hive\n626K\t/dbfs/user\n70M\t/dbfs\n\n=Look for big local files:\n175M\t/root\n2.5G\t/usr\n128K\t/run\n1.9G\t/databricks\n4.0K\t/media\n120K\t/tmp\n16K\t/mnt\n6.4M\t/etc\n4.0K\t/srv\n36K\t/home\n636M\t/var\n4.0K\t/dev\n0\t/proc\n6.8M\t/sbin\n0\t/sys\n4.0K\t/boot\n51M\t/opt\n44M\t/lib\n4.0K\t/lib64\n9.3M\t/bin\n270M\t/local_disk0\n5.6G\t/\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["## PART 2 - Basic Analysis"],"metadata":{}},{"cell_type":"code","source":["# load data into dataframes\ndf_confirm = spark.read.format(\"delta\").load(\"dbfs:/Covid_datasets/Covid_Confirmed\").fillna(0)\ndf_death = spark.read.format(\"delta\").load(\"dbfs:/Covid_datasets/Covid_Deaths\").fillna(0)\ndf_recover = spark.read.format(\"delta\").load(\"dbfs:/Covid_datasets/Covid_Recovered\").fillna(0)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["##### import libraires\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pandas import Series"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["# get all the countries in COuntry_Region column in a list\ndef getCountryList(df):\n  country = df.select('Country_Region').collect()\n  country_list = [country[i][0] for i in range(len(country))]\n  return country_list"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["# get dates - all the date columns we have in dataframe\n#dates_list = spark.sql(\"select * from df_confirm\").drop('Country_Region').columns\ndates_list = df_confirm.drop('Country_Region').columns\n\n# get countries list\ncountries_list = getCountryList(df_confirm)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"code","source":["# create a differenced series with custom interval\ndef difference(dataset):\n    d = dataset.drop('Country_Region').collect()[0][:]\n    return d - Series(d).shift()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["def plotCases(df_c, df_d):\n    \"\"\"\n    function plots daily new cases for few listed countries\n    \"\"\"\n    x = dates_list\n    countries_list = df_c.select('Country_Region').collect()\n    legends = []\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(12,6))\n\n    for i in countries_list: \n        country = i[0][:]\n        legends.append(country)\n        # collect() changes dataframe to row type list, to get list values from row type we convert df to list and then extract just values\n        # collect -> row type list, [0] -> get 0th index row, [:] get all values of row\n        y_c = difference(df_c[df_c['Country_Region']== country])\n        y_d = difference(df_d[df_d['Country_Region']== country])\n        # plot \n        ax1.plot(x,y_c)\n        ax2.plot(x,y_d)\n    ax1.set_title(\"Confirmed Cases\")\n    ax2.set_title(\"Deaths Cases\")\n    xticks = np.arange(0, len(x), step=7)\n    ax1.set_xticks(xticks)\n    ax2.set_xticks(xticks)\n    ax1.set_xticklabels(x[::7], rotation=90)\n    ax2.set_xticklabels(x[::7], rotation=90)\n    ax1.set_xlabel('Weekly dates')\n    ax2.set_xlabel('Weekly dates')\n    ax1.set_ylabel('Population affected')\n    ax2.set_ylabel('Population affected')\n    fig.legend(legends, loc= 'lower left' ,fontsize=16, title = \"Countries\" )\n    fig.tight_layout()\n    fig.suptitle('Daily Covid19 NEW Cases')\n    display(fig)\n    "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["# display confirmed cases for below few countries\ndisp_country = ['US','India','Spain','Italy','United Kingdom']\ndf_c = df_confirm.filter(df_confirm.Country_Region.isin(disp_country))\ndf_d = df_death.filter(df_death.Country_Region.isin(disp_country))\nplotCases(df_c,df_d)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# pie chart cumulative total\n\n# select for most recent date as column, filter countries with zero cases and sort cases \nrecent_col = df_confirm[-1]\ndf_sorted = df_confirm.select('Country_Region',recent_col).sort(recent_col,ascending=False)\n\n# cumulative sum of confirmed cases\ncum_confCase = df_sorted.select(recent_col).groupby().sum().collect()[0][0]\n\n# collect list of 10 countries with most confirmed cases and calculate %age of total cases prevailing in those top countries\nmost_caseConf = df_sorted.collect()[:10]\nper_caseConf = {c[0]:round(c[1]*100/cum_confCase,2) for c in most_caseConf}\ncountry_label = list(per_caseConf.keys())\n\n\n# get death cases for the countries with most confirmed case on recent date\ncum_deathCase = df_death.select(df_death[-1]).groupby().sum().collect()[0][0]\ndf_caseDeath = df_death.select('Country_Region',df_death[-1]) \\\n                .filter(df_death.Country_Region.isin(country_label)).collect()\nper_caseDeath = {c[0]:round(c[1]*100/cum_deathCase,2) for c in df_caseDeath}\n\n\n#plot pie\nfig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(12,6))\nax1.pie([per_caseConf[i] for i in country_label], radius =1.5, autopct='%1.1f%%', shadow=True, startangle=90)\nax2.pie([per_caseDeath[i] for i in country_label], radius = 1.0, autopct='%1.1f%%', shadow=True, startangle=90)\nax1.set_title(\"Highest percent of confirmed cases\")\nax2.set_title(\"death cases\")\nfig.legend(country_label, loc = 'center')\nfig.suptitle(\"Countries with most percentage of cases on %s\"%recent_col._jc.toString(), fontweight='bold')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":26}],"metadata":{"name":"Covid19","notebookId":243668888335827},"nbformat":4,"nbformat_minor":0}
