{"cells":[{"cell_type":"markdown","source":["### PART 1 - Data pipeline with only Spark's 1-Master and 1-Worker node. Run main() to start ETL"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import DataFrame\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport pandas as pd\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["## ETL - get Covid date from date range \ndef end_date():\n  cdate = spark.sql(\"\"\" select current_date() as cdate \"\"\").collect()[0]['cdate'].strftime(\"%m-%d-%Y\") \n  end_date = (datetime.strptime(cdate, '%m-%d-%Y') - timedelta(days=1)).strftime('%m-%d-%Y')\n  return end_date\n\ndef start_date():\n  ## a log file that keeps infrmation of the dates for which data has been successfully download\n  try:\n      dbutils.fs.ls(\"dbfs:/Covid_datasets/log\")\n      max_date = spark.sql(\"SELECT max(*) FROM csv.`dbfs:/Covid_datasets/log`\").collect()\n      start_date = (datetime.strptime(max_date[0][0], '%m-%d-%Y') + timedelta(days=1)).strftime('%m-%d-%Y')\n  except:\n      start_date = \"01-22-2020\"\n  return start_date"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["# date range for which the data willbe pulled from github respository\ndef dateRange():\n  return(pd.date_range(start = start_date(), end = end_date()))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Transform data, create a aggregated sum of cases found grouped Country wise\ndef transform():\n    global df_conf\n    global df_death\n    global df_recover\n    \n    ## Load data into dataframes \n    df_conf = spark.createDataFrame([], schema)\n    df_death = spark.createDataFrame([], schema)\n    df_recover = spark.createDataFrame([], schema)\n    \n    print(\"IN transform\")\n\n    for d in date_range:\n        date = d.strftime(\"%m-%d-%Y\")\n        file = \"file:/tmp/Covid_datasets/\"+str(date)+\".csv\"\n        print('Transform file: ',file)\n        df_data = spark.read.format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat') \\\n                 .option('header','true')\\\n                 .option('inferSchema','true')\\\n                 .load(file)\n    \n        # files dated older than 03-23 has different column name.\n        if 'Country/Region' in df_data.columns:\n            df_data = df_data.withColumnRenamed('Country/Region','Country_Region')\n            \n        try:   \n            df_clean = df_data['Country_Region','Confirmed','Deaths','Recovered']\n        except Exception as e:\n            print(e)\n        \n        df_table = df_clean.groupBy('Country_Region').sum()\n        \n        conf_table = df_table['Country_Region','sum(Confirmed)'].withColumnRenamed('sum(Confirmed)',date)\n        df_conf = df_conf.join(conf_table, on=\"Country_Region\", how='full')\n\n        death_table = df_table['Country_Region','sum(Deaths)'].withColumnRenamed('sum(Deaths)',date)\n        df_death = df_death.join(death_table, on=\"Country_Region\", how='full')\n\n        recover_table = df_table['Country_Region','sum(Recovered)'].withColumnRenamed('sum(Recovered)',date)\n        df_recover = df_recover.join(recover_table, on=\"Country_Region\", how='full')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["def rename_columns(df):\n    old_cols = df.columns\n    new_cols = [f.strip('sum(').strip(')') for f in df.columns]\n    rename = list(zip(old_cols,new_cols))\n    for old,new in rename:\n          df = df.withColumnRenamed(old,new)\n    return df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# store data in some dbfs and add to it the daily updates\ndef load():\n    print(\"IN LOAD\")    \n\n    # The schema/structure of the data is to have a time-series data with dates on columns and each row is a country\n    \n    # command with absolute path\n    df_conf.write.option(\"mergeSchema\",\"true\").format(\"delta\").mode(\"append\").save(\"dbfs:/Covid_datasets/Covid_Confirmed\")\n    df_death.write.option(\"mergeSchema\",\"true\").format(\"delta\").mode(\"append\").save(\"dbfs:/Covid_datasets/Covid_Deaths\")\n    df_recover.write.option(\"mergeSchema\",\"true\").format(\"delta\").mode(\"append\").save(\"dbfs:/Covid_datasets/Covid_Recovered\")\n\n\n    # The above command appends to the table and creates a new row for exisiting Country. \n    # Groupby Countrys and sum the rows and write back to dbfs\n    delta_files = [\"dbfs:/Covid_datasets/Covid_Confirmed\",\"dbfs:/Covid_datasets/Covid_Deaths\",\"dbfs:/Covid_datasets/Covid_Recovered\"]\n    for file in delta_files:\n        df = spark.read.format(\"delta\").load(file).groupby('Country_Region').sum()\n        # suming the rows changes the column names to sum('')\n        df = rename_columns(df)\n        df.write.format(\"delta\").option(\"overwriteSchema\",\"true\").mode(\"overwrite\").save(file)\n    \n    # Above is the temporary solutionNeed to find a better way to update the Streaming data - Use DELTA LAKE\n    # create a batch streaming using delta format and adding to the schema \n    \n    print('Covid_Confirmed, Covid_Deaths and Covid_Recovered tables created')\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["``` We are extracting data from \ninstead of this link https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series\n    .. because above link has cumulative numbers rather than just day by day counts.```"],"metadata":{}},{"cell_type":"code","source":["def extract():\n    # mine/get all .csv files in date range from github daily reports url\n    print('Fetching data to local cluster........')\n    for d in date_range:\n        dated = d.strftime(\"%m-%d-%Y\")\n        file_dated = str(dated) +'.csv'\n        url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/\" + file_dated\n        try:\n          # extract all the files to local disk\n          !wget -P /tmp/Covid_datasets \"$url\"\n          localpath = \"file:/tmp/Covid_datasets/\" + file_dated\n          print('Written file to databricks local filesystem: ', file_dated)          \n        except Exception as e:\n          print(e)       "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["## Initial ETL pipeline \ndef ETL_Pipeline():\n  \n    global date_range\n    date_range = dateRange()\n\n    # mine/get all .csv files in date range from github daily reports url\n    extract()\n\n    # transform/process data from each file\n    transform()\n\n    # load date in specified dataframes\n    load()\n \n    # write to log all the dates whose data is downloaded\n    mylist = list(date_range.map(lambda x: x.strftime(\"%m-%d-%Y\")))\n    spark.createDataFrame(mylist, StringType()).write.format(\"csv\").mode(\"append\").save(\"dbfs:/Covid_datasets/log\")\n\n    # recursively delete all files and folders in the directory releasing memory once copied to dbgs #freeUpFile(file)\n    print(\"Finally, freeing the space by deleting all the files download from git\")\n    dbutils.fs.rm(\"file:/tmp/Covid_datasets\",True) "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["def main():\n    \"\"\"main method starts a pipeline, extracts data, transforms it and loads it into a dbfs client\"\"\"\n    # global empty dataframes\n    global df_conf\n    global df_death\n    global df_recover\n    # schema for the global dataframe\n    schema = StructType([StructField(\"Country_Region\", StringType(), True)])\n\n    ## create a separate folder in dbfs filesystem for Covid19 dataset \n    try:\n      dbutils.fs.ls(\"dbfs:/Covid_datasets\")\n    except:\n      dbutils.fs.mkdirs(\"dbfs:/Covid_datatsets\")\n    #create a new directory in local filesystem as staging area\n    dbutils.fs.rm(\"file:/tmp/Covid_datasets\",True)\n    dbutils.fs.mkdirs(\"file:/tmp/Covid_datasets/\")\n\n    # date range for which the data willbe pulled from github respository\n    global date_range\n\n    # Extract, trasnform and load data in DBFS from github's covid updated data master branch\n    ETL_Pipeline()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["if __name__ == \"__main__\":\n    main()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Fetching data to local cluster........\nIN transform\nIN LOAD\nCovid_Confirmed, Covid_Deaths and Covid_Recovered tables created\nFinally, freeing the space by deleting all the files download from git\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["%sql\nSELECT * FROM delta.`dbfs:/Covid_datasets/Covid_Confirmed` where Country_Region = 'US'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Country_Region</th><th>01-22-2020</th><th>01-23-2020</th><th>01-24-2020</th><th>01-25-2020</th><th>01-26-2020</th><th>01-27-2020</th><th>01-28-2020</th><th>01-29-2020</th><th>01-30-2020</th><th>01-31-2020</th><th>02-01-2020</th><th>02-02-2020</th><th>02-03-2020</th><th>02-04-2020</th><th>02-05-2020</th><th>02-06-2020</th><th>02-07-2020</th><th>02-08-2020</th><th>02-09-2020</th><th>02-10-2020</th><th>02-11-2020</th><th>02-12-2020</th><th>02-13-2020</th><th>02-14-2020</th><th>02-15-2020</th><th>02-16-2020</th><th>02-17-2020</th><th>02-18-2020</th><th>02-19-2020</th><th>02-20-2020</th><th>02-21-2020</th><th>02-22-2020</th><th>02-23-2020</th><th>02-24-2020</th><th>02-25-2020</th><th>02-26-2020</th><th>02-27-2020</th><th>02-28-2020</th><th>02-29-2020</th><th>03-01-2020</th><th>03-02-2020</th><th>03-03-2020</th><th>03-04-2020</th><th>03-05-2020</th><th>03-06-2020</th><th>03-07-2020</th><th>03-08-2020</th><th>03-09-2020</th><th>03-10-2020</th><th>03-11-2020</th><th>03-12-2020</th><th>03-13-2020</th><th>03-14-2020</th><th>03-15-2020</th><th>03-16-2020</th><th>03-17-2020</th><th>03-18-2020</th><th>03-19-2020</th><th>03-20-2020</th><th>03-21-2020</th><th>03-22-2020</th><th>03-23-2020</th><th>03-24-2020</th><th>03-25-2020</th><th>03-26-2020</th><th>03-27-2020</th><th>03-28-2020</th><th>03-29-2020</th><th>03-30-2020</th><th>03-31-2020</th><th>04-01-2020</th><th>04-02-2020</th><th>04-03-2020</th><th>04-04-2020</th></tr></thead><tbody><tr><td>US</td><td>1</td><td>1</td><td>2</td><td>2</td><td>5</td><td>5</td><td>5</td><td>5</td><td>5</td><td>6</td><td>8</td><td>8</td><td>11</td><td>11</td><td>12</td><td>12</td><td>12</td><td>12</td><td>12</td><td>12</td><td>13</td><td>13</td><td>15</td><td>15</td><td>15</td><td>15</td><td>15</td><td>15</td><td>15</td><td>15</td><td>35</td><td>35</td><td>35</td><td>53</td><td>53</td><td>59</td><td>60</td><td>62</td><td>70</td><td>76</td><td>101</td><td>122</td><td>153</td><td>221</td><td>278</td><td>417</td><td>537</td><td>605</td><td>959</td><td>1281</td><td>1663</td><td>2179</td><td>2726</td><td>3499</td><td>4632</td><td>6421</td><td>7786</td><td>13680</td><td>19101</td><td>25493</td><td>33746</td><td>43667</td><td>53740</td><td>65778</td><td>83836</td><td>101657</td><td>121478</td><td>140886</td><td>161807</td><td>188172</td><td>213372</td><td>243453</td><td>275586</td><td>308850</td></tr></tbody></table></div>"]}}],"execution_count":13},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":15}],"metadata":{"name":"Covid19","notebookId":243668888335827},"nbformat":4,"nbformat_minor":0}
